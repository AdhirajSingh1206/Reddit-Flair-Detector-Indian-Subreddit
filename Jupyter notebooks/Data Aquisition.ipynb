{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition and Cleaning:\n",
    "\n",
    "This notebook demonstrates the scraping of data from the r/india subreddit using Praw Module in python.\n",
    "PRAW which stands for Python Reddit API Wrapper helps us scrape data sceamlessly from any subreddit.\n",
    "\n",
    "Post scraping the data, we saves the data in a csv file for further processing using Pandas Module.\n",
    "\n",
    "We further load this raw Data again using Pandas and preform several cleaning and regularisation techniques and save it back into a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAuth with PRAW:\n",
    "Setting up Open-Standard Authorization Protocol for the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"ffKcEa2xKfnhyg\", client_secret=\"IJqQkTrDio0xKsKYKYmgeWSoOLM\",\n",
    "    user_agent=\"flair_predication\", username=\"ASingh1206\", password=\"g5gh#4$iQFGNBad\")\n",
    "\n",
    "subreddit = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping:\n",
    "Here we declare the various Flairs and setup code to efficiently receive data from the <b>API into the Pandas Dataframe</b>, further onces this raw data is collected it is then stored into a <b>CSV file</b>.\n",
    "\n",
    "The API delivers around <b>240 Posts per Flair</b> on an average. Along with this the <b>Top 5 comments</b> per post have been also been saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = [\"Photography\", \"Science/Technology\",\"AskIndia\",\"Business/Finance\", \"Policy/Economy\",\n",
    "          \"Sports\", \"Food\", \"Politics\", \"Scheduled\", \"Non-Political\"]\n",
    "        \n",
    "title = [] \n",
    "score = []\n",
    "url = []\n",
    "body = []\n",
    "author = []\n",
    "fl = []\n",
    "com = []\n",
    "num = []\n",
    "\n",
    "for flair in flairs:\n",
    "    relevant_subreddits = subreddit.search(f\"flair_name:{flair}\",limit=300)\n",
    "    series = 0\n",
    "    for submission in relevant_subreddits:\n",
    "        count = 0\n",
    "        if submission.num_comments != 0:\n",
    "                count = min(5, submission.num_comments)\n",
    "                submission.comments.replace_more(count)\n",
    "                comment = ''\n",
    "                for top_level_comment in submission.comments:\n",
    "                    comment = comment + ' ' + top_level_comment.body\n",
    "\n",
    "        print(flair, \"  \", series)\n",
    "        series = series + 1\n",
    "\n",
    "        title.append(submission.title)\n",
    "        score.append(submission.score)\n",
    "        url.append(submission.url)\n",
    "        body.append(submission.selftext)\n",
    "        author.append(submission.author)\n",
    "        fl.append(flair)\n",
    "        com.append(comment)\n",
    "        num.append(submission.num_comments)\n",
    "    \n",
    "dict = {'title': title, 'author': author, 'url': url, 'body': body, 'score': score,'flair': fl, 'num': num, 'comments' : com}  \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "df.fillna(\"\",inplace = True) \n",
    "\n",
    "# saving the dataframe \n",
    "df.to_csv('f_300.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data for a Preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape--> (2422, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>flair</th>\n",
       "      <th>num</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Different stages of hair loss in perfect order...</td>\n",
       "      <td>BreakingBrownBread</td>\n",
       "      <td>https://i.redd.it/ydbmwsa7jpt41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2802</td>\n",
       "      <td>Photography</td>\n",
       "      <td>88</td>\n",
       "      <td>So, can I guess that you're as bald as Shakaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women gather together during Dust storm in Raj...</td>\n",
       "      <td>TheDosaMan</td>\n",
       "      <td>https://i.redd.it/uapdc9dvels41.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3565</td>\n",
       "      <td>Photography</td>\n",
       "      <td>74</td>\n",
       "      <td>Steve McCurry captured this stunning image in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zoom in! I took over 600 shots of last night's...</td>\n",
       "      <td>vpsj</td>\n",
       "      <td>https://i.imgur.com/RLL0xvH.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1463</td>\n",
       "      <td>Photography</td>\n",
       "      <td>81</td>\n",
       "      <td>#Details:\\n\\nFirst of all, please note that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Wild Gaur, Nagarahole National Park</td>\n",
       "      <td>Coconut_Kid</td>\n",
       "      <td>https://i.redd.it/1zz6atjncds41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>658</td>\n",
       "      <td>Photography</td>\n",
       "      <td>71</td>\n",
       "      <td>In wild there is probably no Majestic beast a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Everyone, Puffy the Superdog. (ZenFone 6)</td>\n",
       "      <td>bosama_in_laden</td>\n",
       "      <td>https://i.redd.it/bk0fba8havt41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603</td>\n",
       "      <td>Photography</td>\n",
       "      <td>39</td>\n",
       "      <td>Must be nice having trees around your house. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "0  Different stages of hair loss in perfect order...  BreakingBrownBread   \n",
       "1  Women gather together during Dust storm in Raj...          TheDosaMan   \n",
       "2  Zoom in! I took over 600 shots of last night's...                vpsj   \n",
       "3              A Wild Gaur, Nagarahole National Park         Coconut_Kid   \n",
       "4          Everyone, Puffy the Superdog. (ZenFone 6)     bosama_in_laden   \n",
       "\n",
       "                                   url body  score        flair  num  \\\n",
       "0  https://i.redd.it/ydbmwsa7jpt41.jpg  NaN   2802  Photography   88   \n",
       "1  https://i.redd.it/uapdc9dvels41.png  NaN   3565  Photography   74   \n",
       "2      https://i.imgur.com/RLL0xvH.jpg  NaN   1463  Photography   81   \n",
       "3  https://i.redd.it/1zz6atjncds41.jpg  NaN    658  Photography   71   \n",
       "4  https://i.redd.it/bk0fba8havt41.jpg  NaN    603  Photography   39   \n",
       "\n",
       "                                            comments  \n",
       "0   So, can I guess that you're as bald as Shakaa...  \n",
       "1   Steve McCurry captured this stunning image in...  \n",
       "2   #Details:\\n\\nFirst of all, please note that t...  \n",
       "3   In wild there is probably no Majestic beast a...  \n",
       "4   Must be nice having trees around your house. ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('f_300.csv')\n",
    "print(\"Shape-->\", dff.shape)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function for regularize the data:\n",
    "Here the data is made uniform in nature and any kind of StopWords have also been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbol = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = replace_by_space.sub(' ', text) \n",
    "    text = replace_symbol.sub('', text) \n",
    "    text = text.lower() \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def cclean_text(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alteration to the data:\n",
    "This step was performed after we had conducted the Exploratory Data Analysis on the the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff = dff.drop(\"score\", axis=1)\n",
    "#dff = dff.drop(\"url\", axis=1)\n",
    "#dff = dff.drop(\"num\", axis=1)\n",
    "\n",
    "dff['title'] = dff['title'].apply(cclean_text)\n",
    "dff['body'] = dff['body'].apply(cclean_text)\n",
    "dff['comments'] = dff['comments'].apply(cclean_text)\n",
    "\n",
    "dff = dff.fillna('')\n",
    "\n",
    "combined_features = dff[\"title\"] + \". \" + dff[\"body\"] + \". \" + dff[\"comments\"] \n",
    "dff = dff.assign(combined_features = combined_features)\n",
    "\n",
    "#saving the csv file\n",
    "\n",
    "dff.to_csv('f_300_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Data post Regularizing:\n",
    "This data is now ready to be used to train and test our models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2422, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>flair</th>\n",
       "      <th>num</th>\n",
       "      <th>comments</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>differ stage hair loss perfect order mumbai lo...</td>\n",
       "      <td>BreakingBrownBread</td>\n",
       "      <td>https://i.redd.it/ydbmwsa7jpt41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2802</td>\n",
       "      <td>Photography</td>\n",
       "      <td>88</td>\n",
       "      <td>guess bald shakaal hei gui rememb train hairst...</td>\n",
       "      <td>differ stage hair loss perfect order mumbai lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>women gather dust storm rajasthan</td>\n",
       "      <td>TheDosaMan</td>\n",
       "      <td>https://i.redd.it/uapdc9dvels41.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3565</td>\n",
       "      <td>Photography</td>\n",
       "      <td>74</td>\n",
       "      <td>steve mccurri captur stun imag delet steven mc...</td>\n",
       "      <td>women gather dust storm rajasthan. nan. steve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zoom took shot night supermoon stack detail lu...</td>\n",
       "      <td>vpsj</td>\n",
       "      <td>https://i.imgur.com/RLL0xvH.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1463</td>\n",
       "      <td>Photography</td>\n",
       "      <td>81</td>\n",
       "      <td>detail note composit shot mean star moon shot ...</td>\n",
       "      <td>zoom took shot night supermoon stack detail lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wild gaur nagarahol nation park</td>\n",
       "      <td>Coconut_Kid</td>\n",
       "      <td>https://i.redd.it/1zz6atjncds41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>658</td>\n",
       "      <td>Photography</td>\n",
       "      <td>71</td>\n",
       "      <td>wild probabl majest beast beauti gaur photo cr...</td>\n",
       "      <td>wild gaur nagarahol nation park. nan. wild pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>puffi superdog zenfon</td>\n",
       "      <td>bosama_in_laden</td>\n",
       "      <td>https://i.redd.it/bk0fba8havt41.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603</td>\n",
       "      <td>Photography</td>\n",
       "      <td>39</td>\n",
       "      <td>nice have tree hous free space live like actua...</td>\n",
       "      <td>puffi superdog zenfon. nan. nice have tree hou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "0  differ stage hair loss perfect order mumbai lo...  BreakingBrownBread   \n",
       "1                  women gather dust storm rajasthan          TheDosaMan   \n",
       "2  zoom took shot night supermoon stack detail lu...                vpsj   \n",
       "3                    wild gaur nagarahol nation park         Coconut_Kid   \n",
       "4                              puffi superdog zenfon     bosama_in_laden   \n",
       "\n",
       "                                   url body  score        flair  num  \\\n",
       "0  https://i.redd.it/ydbmwsa7jpt41.jpg  NaN   2802  Photography   88   \n",
       "1  https://i.redd.it/uapdc9dvels41.png  NaN   3565  Photography   74   \n",
       "2      https://i.imgur.com/RLL0xvH.jpg  NaN   1463  Photography   81   \n",
       "3  https://i.redd.it/1zz6atjncds41.jpg  NaN    658  Photography   71   \n",
       "4  https://i.redd.it/bk0fba8havt41.jpg  NaN    603  Photography   39   \n",
       "\n",
       "                                            comments  \\\n",
       "0  guess bald shakaal hei gui rememb train hairst...   \n",
       "1  steve mccurri captur stun imag delet steven mc...   \n",
       "2  detail note composit shot mean star moon shot ...   \n",
       "3  wild probabl majest beast beauti gaur photo cr...   \n",
       "4  nice have tree hous free space live like actua...   \n",
       "\n",
       "                                   combined_features  \n",
       "0  differ stage hair loss perfect order mumbai lo...  \n",
       "1  women gather dust storm rajasthan. nan. steve ...  \n",
       "2  zoom took shot night supermoon stack detail lu...  \n",
       "3  wild gaur nagarahol nation park. nan. wild pro...  \n",
       "4  puffi superdog zenfon. nan. nice have tree hou...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('f_300_clean.csv')\n",
    "print(dff.shape)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
